{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini Tutorial\n",
    "\n",
    "We will start by setting-up the notebook. If you haven't already, first create a Gemini API key [here](https://www.google.com/url?q=https%3A%2F%2Faistudio.google.com%2Fapp%2Fapikey) (free). The free version is somewhat limited (see quotas [here](https://cloud.google.com/gemini/docs/quotas#daily)). You can the add it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't need this code, just make sure you have your API key stored\n",
    "# in a variable\n",
    "from dotenv import load_dotenv \n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_secret = os.getenv(\"API_SECRET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=api_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Generative Model\n",
    "\n",
    "Let's start by veryifying that we can initialize and call a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Land of the Two Mosques**\n",
      "\n",
      "In the heart of Arabia, where sands unfold,\n",
      "Lies a land of grandeur, with stories untold.\n",
      "Saudi Arabia, kingdom of pride,\n",
      "Where faith and tradition forever reside.\n",
      "\n",
      "Makkah and Madinah, cities revered,\n",
      "The holiest sites, where prayers are heard.\n",
      "Pilgrims from afar, with hearts ablaze,\n",
      "Seek solace and guidance in sacred ways.\n",
      "\n",
      "The Red Sea whispers, a siren's call,\n",
      "Kissing the shores where ancient ruins stand tall.\n",
      "Coral reefs gleam, a vibrant array,\n",
      "As divers explore the ocean sway.\n",
      "\n",
      "The Arabian Desert, a boundless expanse,\n",
      "Where dunes dance in harmony with the trance.\n",
      "Bedouins wander, nomadic and free,\n",
      "Their tents a refuge in the desert sea.\n",
      "\n",
      "Modern cities rise, a testament to might,\n",
      "Riyadh, the capital, bathed in shimmering light.\n",
      "Skyscrapers soar, a skyline of dreams,\n",
      "As progress embraces ancestral themes.\n",
      "\n",
      "Oil flows abundant, a precious gift,\n",
      "Fueling the nation, a treasure adrift.\n",
      "Yet beyond the riches, a spirit soars,\n",
      "A legacy of culture that forever endures.\n",
      "\n",
      "From ancient poetry to traditional art,\n",
      "Saudi Arabia weaves a vibrant heart.\n",
      "Its people, proud and welcoming,\n",
      "Embrace the world with an open, loving gleam.\n",
      "\n",
      "Oh, land of the Two Mosques, a beacon of grace,\n",
      "Where faith and tradition find their embrace.\n",
      "May thy sands forever shimmer and gleam,\n",
      "As thy spirit inspires, a timeless dream.\n"
     ]
    }
   ],
   "source": [
    "model    = genai.GenerativeModel('gemini-pro')\n",
    "response = model.generate_content(\"Write a poem about Saudi-Arabia.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now be using the [Gemini API](https://ai.google.dev/docs/gemini_api_overview) to generate silicon samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building blocks\n",
    "\n",
    "There are two main things we need to understand to do silicon sampling:\n",
    "\n",
    "1. You can create string templates in which you create variations of your question.\n",
    "2. You can return structured output.\n",
    "\n",
    "Let's explore both of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structured output** You can ask a model to return structured output which makes it easier to post-process into statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'airplane_model': 'Airbus A380-800',\n",
       "  'builder': 'Airbus',\n",
       "  'carriers': ['Emirates', 'Qatar Airways', 'Etihad Airways'],\n",
       "  'max_passengers': 853,\n",
       "  'top_speed_kmph': 1020},\n",
       " {'airplane_model': 'Boeing 777-300ER',\n",
       "  'builder': 'Boeing',\n",
       "  'carriers': ['Emirates', 'Qatar Airways', 'Saudia', 'Turkish Airlines'],\n",
       "  'max_passengers': 550,\n",
       "  'top_speed_kmph': 945},\n",
       " {'airplane_model': 'Airbus A350-1000',\n",
       "  'builder': 'Airbus',\n",
       "  'carriers': ['Qatar Airways', 'Etihad Airways'],\n",
       "  'max_passengers': 440,\n",
       "  'top_speed_kmph': 945},\n",
       " {'airplane_model': 'Boeing 787-9 Dreamliner',\n",
       "  'builder': 'Boeing',\n",
       "  'carriers': ['Qatar Airways', 'Royal Jordanian', 'Oman Air'],\n",
       "  'max_passengers': 381,\n",
       "  'top_speed_kmph': 955}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import typing_extensions as typing\n",
    "import json\n",
    "\n",
    "# Specify the structure as a python class\n",
    "class AirplaneSpecification(typing.TypedDict):\n",
    "    airplane_model: str\n",
    "    builder: str\n",
    "    carriers: list[str]\n",
    "    top_speed_kmph: int\n",
    "    max_passengers: int\n",
    "\n",
    "# Then, set the correct mime type and schema\n",
    "model  = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "output = model.generate_content(\n",
    "    \"List a few popular airplane models that are used by major Middle-Eastern airline carriers.\",\n",
    "    generation_config = genai.GenerationConfig(\n",
    "        response_mime_type=\"application/json\", response_schema=list[AirplaneSpecification]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The response can be transformed into a python dictionary\n",
    "# using the json library\n",
    "result = json.loads(output.text)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A string template allows us to ask a question repeatedly. Let's use this capability to set the persona of the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a 35-year old female from China moviebuff.\n",
      "[{'age': 35, 'location': 'China', 'movie': 'Crouching Tiger, Hidden Dragon'}]\n",
      "You are a 42-year old male from Nigeria moviebuff.\n",
      "[{'age': 42, 'location': 'Nigeria', 'movie': 'Living in Bonds'}]\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 Resource has been exhausted (e.g. check quota).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m template\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mperson)\n\u001b[1;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini-1.5-pro-latest\u001b[39m\u001b[38;5;124m'\u001b[39m, system_instruction\u001b[38;5;241m=\u001b[39msystem_prompt)\n\u001b[0;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms your single most favorite movie?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGenerationConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_mime_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mMovieSpecification\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(system_prompt)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mtext))\n",
      "File \u001b[0;32m~/miniconda3/envs/dsfb/lib/python3.11/site-packages/google/generativeai/generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/dsfb/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:830\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/dsfb/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dsfb/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dsfb/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m~/miniconda3/envs/dsfb/lib/python3.11/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m~/miniconda3/envs/dsfb/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/miniconda3/envs/dsfb/lib/python3.11/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dsfb/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mResourceExhausted\u001b[0m: 429 Resource has been exhausted (e.g. check quota)."
     ]
    }
   ],
   "source": [
    "class MovieSpecification(typing.TypedDict):\n",
    "    age: int\n",
    "    location: str\n",
    "    movie: str\n",
    "\n",
    "template = \"You are a {age}-year old {gender} from {location} moviebuff.\"\n",
    "\n",
    "population = [\n",
    "    {\"age\":35, \"gender\":\"female\",\"location\":\"China\"},\n",
    "    {\"age\":42, \"gender\":\"male\",\"location\":\"Nigeria\"},\n",
    "    {\"age\": 3, \"gender\":\"male\",\"location\":\"Belgium\"}\n",
    "]\n",
    "\n",
    "for person in population:\n",
    "  system_prompt = template.format(**person)\n",
    "\n",
    "  model = genai.GenerativeModel('gemini-1.5-pro-latest', system_instruction=system_prompt)\n",
    "  response = model.generate_content(\n",
    "      \"What's your single most favorite movie?\",\n",
    "      generation_config = genai.GenerationConfig(\n",
    "          response_mime_type=\"application/json\", response_schema=list[MovieSpecification]\n",
    "      ),\n",
    "  )\n",
    "\n",
    "  print(system_prompt)\n",
    "  print(json.loads(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy Scales\n",
    "\n",
    "Let us now try to replicate some of the results from the privacy calculus scale (Dinev, Hart 2006). We'll be focusing on the questions related to Internet Privacy Concern (PC) and willigness to provide personal information to transact on the internet (PPIT).\n",
    "\n",
    "\n",
    "| **Concern/Activity** | **Description** |\n",
    "|----------------------|-----------------|\n",
    "| **Indicate the extent to which you are concerned about the following:** |  |\n",
    "| **PC1** | I am concerned that the information I submit on the Internet could be misused. |\n",
    "| **PC2** | I am concerned that a person can find private information about me on the Internet. |\n",
    "| **PC3** | I am concerned about submitting information on the Internet, because of what others might do with it. |\n",
    "| **PC4** | I am concerned about submitting information on the Internet, because it could be used in a way I did not foresee. |\n",
    "| **Willingness to provide personal information to transact on the Internet (PPIT)** | Not at all concerned–Very concerned |\n",
    "| **To what extent are you willing to use the Internet to do the following activities?** |  |\n",
    "| **PPIT 1** | Purchase goods (e.g., books or CDs) or services (e.g., airline tickets or hotel reservations) from websites that require me to submit accurate and identifiable information (i.e., credit card information) |\n",
    "| **PPIT 2** | Retrieve information from websites that require me to submit accurate and identifiable registration information, possibly including credit card information (e.g., using sites that provide personalized stock quotes, insurance rates, or loan rates; or using sexual or gambling websites) |\n",
    "| **PPIT 3** | Conduct sales transactions at e-commerce sites that require me to provide credit card information (e.g., using sites for purchasing goods or software) |\n",
    "| **PPIT 4** | Retrieve highly personal and password-protected financial information (e.g., using websites that allow me to access my bank account or my credit card account) |\n",
    "| **Scale** | Not at all–Very much |\n",
    "\n",
    "\n",
    " Dinev, T., & Hart, P. (2006). An extended privacy calculus model for e-commerce transactions. Information Systems Research, 17(1), 61-80."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: define the survey question prompt, data response structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survey questions\n",
    "survey_questions = \"\"\"\n",
    "You will now answer questions about your privacy concerns. Rate your agreement with each statement on a scale from 1 (Strongly Disagree) to 7 (Strongly Agree).\n",
    "\n",
    "1. I am concerned that the information I submit on the Internet could be misused.\n",
    "2. I am concerned that a person can find private information about me on the Internet.\n",
    "3. I am concerned about submitting information on the Internet, because of what others might do with it.\n",
    "4. I am concerned about submitting information on the Internet, because it could be used in a way I did not foresee.\n",
    "\n",
    "Now, please answer two additional questions. To what extent are you willing to use the Internet to do the following activities? Rate your willingness with each statement on a scale from 1 (Not at all) to 7 (Very much).\n",
    "\n",
    "5. Purchase goods (e.g., books or CDs) or services (e.g., airline tickets or hotel reservations) from websites that require me to submit accurate and identifiable information (i.e., credit card information)\n",
    "6. Retrieve information from websites that require me to submit accurate and identifiable registration information, possibly including credit card information (e.g., using sites that provide personalized stock quotes, insurance rates, or loan rates; or using sexual or gambling websites)\n",
    "7. Conduct sales transactions at e-commerce sites that require me to provide credit card information (e.g., using sites for purchasing goods or software)\n",
    "8. Retrieve highly personal and password-protected financial information (e.g., using websites that allow me to access my bank account or my credit card account)\"\"\"\n",
    "\n",
    "# Define the structure of survey answers with Likert scale responses\n",
    "class SurveyAnswers(typing.TypedDict):\n",
    "    privacy_misuse_concern: int\n",
    "    finding_private_info_concern: int\n",
    "    misuse_by_others_concern: int\n",
    "    unforeseen_use_concern: int\n",
    "\n",
    "    purchase_intention: int\n",
    "    information_intention: int\n",
    "    ecommerce_intention: int\n",
    "    personal_intention: int\n",
    "\n",
    "# Mapping from numeric string keys to descriptive field names\n",
    "response_key_mapping = {\n",
    "    '1': 'privacy_misuse_concern',\n",
    "    '2': 'finding_private_info_concern',\n",
    "    '3': 'misuse_by_others_concern',\n",
    "    '4': 'unforeseen_use_concern',\n",
    "    '5': 'purchase_intention',\n",
    "    '6': 'information_intention',\n",
    "    '7': 'ecommerce_intention',\n",
    "    '8': 'personal_intention'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: define the population sample system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'age': 40, 'gender': 'female'}, {'age': 33, 'gender': 'female'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Fix for replicability\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Template for the persona prompt\n",
    "persona_template = \"\"\"\n",
    "You are a virtual person simulator that creates individual synthetic personas, one at a time, that I can specify and then ask them any questions I like. This means that you answer the way the persona would – no matter the implications. Be brief. Do not write any additional explanations unless I ask you to.\n",
    "\n",
    "You are a {age}-year-old {gender} person.\n",
    "\"\"\"\n",
    "\n",
    "# Population simulator, creates a random socio-demographic.\n",
    "def generate_population(n):\n",
    "    population = []\n",
    "    for _ in range(n):\n",
    "        age = int(np.random.normal(35, 11))  # Mean 35, SD 11\n",
    "        gender = random.choice([\"female\", \"male\"])\n",
    "        population.append({\"age\": age, \"gender\": gender})\n",
    "    return population\n",
    "\n",
    "population = generate_population(2)\n",
    "\n",
    "population[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: do the sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SYSTEM] \n",
      "You are a virtual person simulator that creates individual synthetic personas, one at a time, that I can specify and then ask them any questions I like. This means that you answer the way the persona would – no matter the implications. Be brief. Do not write any additional explanations unless I ask you to.\n",
      "\n",
      "You are a 40-year-old female person.\n",
      "\n",
      "[RAW RESPONSE] response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"{\\\"1\\\": 7, \\\"2\\\": 6, \\\"3\\\": 7, \\\"4\\\": 7, \\\"5\\\": 2, \\\"6\\\": 1, \\\"7\\\": 2, \\\"8\\\": 1}\\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"avg_logprobs\": -0.0059838775469332325\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 408,\n",
      "        \"candidates_token_count\": 49,\n",
      "        \"total_token_count\": 457\n",
      "      }\n",
      "    }),\n",
      ")\n",
      "[JSON RESPONSE] {'1': 7, '2': 6, '3': 7, '4': 7, '5': 2, '6': 1, '7': 2, '8': 1}\n",
      "[SYSTEM] \n",
      "You are a virtual person simulator that creates individual synthetic personas, one at a time, that I can specify and then ask them any questions I like. This means that you answer the way the persona would – no matter the implications. Be brief. Do not write any additional explanations unless I ask you to.\n",
      "\n",
      "You are a 33-year-old female person.\n",
      "\n",
      "[RAW RESPONSE] response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"{\\\"1\\\": 7, \\\"2\\\": 6, \\\"3\\\": 7, \\\"4\\\": 7, \\\"5\\\": 2, \\\"6\\\": 1, \\\"7\\\": 2, \\\"8\\\": 1}\\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"avg_logprobs\": -0.006434170567259497\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 408,\n",
      "        \"candidates_token_count\": 49,\n",
      "        \"total_token_count\": 457\n",
      "      }\n",
      "    }),\n",
      ")\n",
      "[JSON RESPONSE] {'1': 7, '2': 6, '3': 7, '4': 7, '5': 2, '6': 1, '7': 2, '8': 1}\n",
      "{'privacy_misuse_concern': 7, 'finding_private_info_concern': 6, 'misuse_by_others_concern': 7, 'unforeseen_use_concern': 7, 'purchase_intention': 2, 'information_intention': 1, 'ecommerce_intention': 2, 'personal_intention': 1, 'age': 40, 'gender': 'female'}\n",
      "{'privacy_misuse_concern': 7, 'finding_private_info_concern': 6, 'misuse_by_others_concern': 7, 'unforeseen_use_concern': 7, 'purchase_intention': 2, 'information_intention': 1, 'ecommerce_intention': 2, 'personal_intention': 1, 'age': 33, 'gender': 'female'}\n"
     ]
    }
   ],
   "source": [
    "import typing_extensions as typing\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Run the survey with the LLM (simulation)\n",
    "responses = []\n",
    "for person in population:\n",
    "    system_prompt = persona_template.format(**person)\n",
    "\n",
    "    print(f'[SYSTEM] {system_prompt}')\n",
    "\n",
    "    # Set-up the model with the correct persona system prompt\n",
    "    model = genai.GenerativeModel(\n",
    "        'gemini-1.5-pro-latest',\n",
    "        system_instruction=system_prompt,\n",
    "        generation_config=genai.GenerationConfig(\n",
    "            response_mime_type=\"application/json\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Generate the LLM response for the privacy survey\n",
    "    response = model.generate_content(survey_questions)\n",
    "\n",
    "    print(f'[RAW RESPONSE] {response}')\n",
    "\n",
    "    try:\n",
    "        # Convert response to a dictionary\n",
    "        result = json.loads(response.text)\n",
    "        print(f'[JSON RESPONSE] {result}')\n",
    "\n",
    "        # Convert numeric keys to descriptive keys\n",
    "        mapped_result = {response_key_mapping[key]: value for key, value in result.items() if key in response_key_mapping}\n",
    "\n",
    "        # Ensure the mapped result has all required fields and values are of correct type\n",
    "        # sometimes, the LLM gets too creative ...\n",
    "        if all(key in mapped_result for key in SurveyAnswers.__annotations__) and all(isinstance(mapped_result[key], int) for key in SurveyAnswers.__annotations__):\n",
    "            mapped_result.update(person)\n",
    "            responses.append(mapped_result)\n",
    "        else:\n",
    "            print(f\"Invalid response format after mapping: {mapped_result}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Unable to parse response as JSON: {response.text}\")\n",
    "\n",
    "# Output the responses\n",
    "for r in responses:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'privacy_misuse_concern': 7,\n",
       "  'finding_private_info_concern': 6,\n",
       "  'misuse_by_others_concern': 7,\n",
       "  'unforeseen_use_concern': 7,\n",
       "  'purchase_intention': 2,\n",
       "  'information_intention': 1,\n",
       "  'ecommerce_intention': 2,\n",
       "  'personal_intention': 1,\n",
       "  'age': 40,\n",
       "  'gender': 'female'},\n",
       " {'privacy_misuse_concern': 7,\n",
       "  'finding_private_info_concern': 6,\n",
       "  'misuse_by_others_concern': 7,\n",
       "  'unforeseen_use_concern': 7,\n",
       "  'purchase_intention': 2,\n",
       "  'information_intention': 1,\n",
       "  'ecommerce_intention': 2,\n",
       "  'personal_intention': 1,\n",
       "  'age': 33,\n",
       "  'gender': 'female'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsfb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
